\chapter{Conclusions}
\label{cha:conclusions}

The objective of this work was to develop a model capable of identifying and classifying the symptoms within a patient's statement.

In particular, as shown in chapter \ref{cha:results}, the Symptom Identifier can identify, using BERT, the $88.0 \%$ of the symptoms present in the sentences. However, in the meantime, it generates a considerable number of non-sense tokens for predictions which are detrimental because they will be inputs of the next component: the Symptom Classifier.

The Symptom Classifier takes as input the tokens for predictions originated by the previous component, of which $84.8 \%$ are correct (redundant and non-redundant), and outputs the CUI predictions based on them. Considering that the percentage of correct predictions is $48.0 \%$, it is clear that the mapping is the main sub-component to perfect in order to improve the performance of the model.

However, the results are overall satisfactory because even using an extremely simple vector representation of symptoms (the mean of the embeddings of the words inside the concept name of the symptom), the model reaches an accuracy of $58.7 \%$ and a percentage of correct predictions of $48.0 \%$, doing 1.5 medium attempts for each real symptom in the sentences \footnote{Results achieved using BERT for QA, GloVe 200, minimum similarity level set at 0.8, Body Part Finder and ``pruning'' enabled.}.

The fact that these results depend strictly on the vector representations of symptoms shows that there is much scope for improvement. For example, a weakness of the vector representations of symptoms is that of being biased to the medical language (they were computed using concept names that are inside a medical classification). 

\section{Further developments}

\subsection*{Possible improvements}
To improve the quality of the symptom representations can be worth to use, instead, the vectorization of symptom names expressed with the same language of a common patient. This could be useful in order to move closer linguistically and semantically to the problem that the patient wants to express.

A more radical modification of the last idea (but extremely difficult to realize) is that of creating a patient level classification of symptoms (independent from any language) and associating to each concept in this classification a list of expressions, which are the synonyms of the concept. For doing so it is necessary to collect a large quantity of patient's sentences, study how they express their problems and what is their common knowledge (and choose, of course, what is common knowledge and what is not).

% use contextual embeddings
In order to address the problem of word sense disambiguation is possible to adopt contextual embeddings. BERT embeddings are contextual; however, they did not outperform GloVe because they are not fine-tuned to a specific domain.

\subsection*{Future applications}
As mentioned in section \ref{sub:whyclassifying}, this work can be considered as a basis for a future component aimed at finding related symptoms and asking questions about them. This can be done because the component discussed in this work maps the symptoms present in the sentence towards a standard classification, enabling future components to use different symptoms representations.

A possible approach consists in using \texttt{cui2vec} \cite{cui2vec}, a new set of embeddings for medical concepts learned using a large collection of multimodal medical data. These internal representations cover the majority of the UMLS concepts and can be used for finding related symptoms: for example, using them as inputs for a clustering algorithm.


% Last improvement...
% use compositional syntax

    %%%%%%% USEFUL FOR FUTURE WORK
    % For example, there is no explicit concept for a "third degree burn of left index finger caused by hot water". However, using the compositional syntax it can be represented as:

    % \begin{verbatim}
    %    284196006 | burn of skin | :
    %    116676008 | associated morphology | = 80247002 | third degree burn injury |
    %  , 272741003 | laterality | = 7771000 | left |
    %  , 246075003 | causative agent | = 47448006 | hot water |
    %  , 363698007 | finding site | = 83738005 | index finger structure
    % \end{verbatim}
    %
    % BACK is both a preposition and a body part, then using contextual embeddings like bert finetuned or elmo embeddings could provide better results
    %%%%%%%