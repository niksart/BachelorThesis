\begin{figure}[h]
\centering
\includegraphics[width=17cm]{methods_overview}
\caption{Overview of methods}
\medskip
\end{figure}

\newpage
\chapter{Methods}
\label{cha:methods}
In this chapter, firstly, the Natural Language Processing methods used in this work are described in detail. Secondly, the abstract components are analyzed.

\section{The \textit{Body Part Finder}}
\label{sec:body_part_finder}
\subsection{The body part list}
The objective of this component is to identify body parts words within the patient's sentence. For this goal, it is necessary to have a list of body parts. In addition, each body part can be expressed by the patient using a large variety of terms: its synonyms. Hence, with the addition of an abstraction layer, the list of body parts becomes a list of body parts concepts. Each body part concept is then associated to a list of its synonyms. 

Another problem to face is that the language of the body parts classification should be understandable by every patient: therefore, it has to be informal and not strictly technical.

At the moment, it is not available any body part classification with these features. Therefore, a new classification has been produced by the author. This classification is based mainly on the list of body parts found in \footnote{https://en.wikipedia.org/wiki/Glossary\_of\_medicine} \cite{bodypartswiki}. The classification is saved in a \begin{verbatim}.csv\end{verbatim} file.

\subsection{Finding a body part in a sentence}
Finding a body part in a sentence is trivial if it is available an exhaustive list of body parts and the respective synonyms. The code does a simple string matching between each word in the sentence and each synonym present inside the classification. This approach has some drawbacks: for example, the word “back” can be both a preposition and a body part. Therefore, taking in consideration only words may lead to forget the context: this might introduce misunderstandings lowering the quality of the results.

\section{Question Answering system}
\label{sec:qa_system}
The outcome of the \textit{Body Part Finder} is then passed to the \textit{QA system}. Its aim is highlighting, in the patient's sentence, the words that answers to a set of questions like \textit{What are the symptoms in the sentence?} or \textit{What is the problem with the neck?}. This task is called in literature \textit{Machine Reading Comprehension} \cite{rnet}.

\subsection{Types of questions}
The questions asked to the specified model over each sentence can be of two types:
\begin{enumerate}
  \item generic question about the symptoms in the sentence: \textit{What are the symptoms?}
  \item specific question about the problem of a found body part in the sentence: \textit{What is the problem with the }\texttt{[body part]}?
\end{enumerate}

The question over the symptoms is intentionally generic because its scope is to catch the majority of the symptoms within the sentence. Thus, as shown in section \ref{sec:answer_interpreter}, the answer to this question must be interpreted.

At the same time, the question over a body part is deliberately specific. The logic behind this question is that if a patient refers to a body part during the interview, can be the source of a problem.

Figure \ref{fig:qa} shows the whole component.

%% IMAGE ABOUT QA SYSTEM
\begin{figure}[h]
\centering
\includegraphics[width=14cm]{qa_sys}
\caption{The \textit{QA system} illustated.}
\medskip
\label{fig:qa}
\end{figure}

As shown in Figure \ref{fig:qa}, two problems may arise: 
\begin{enumerate}
  \item the model can sometimes provide an useful answer but too generic (this is the case of \textit{small bumps})
  \item it might provide redundant answers, increasing unnecessarily the number of predictions.
\end{enumerate}

\subsection{BERT}
\input{bert_explained}

\subsection{R-NET}
\input{rnet_explained}


\section{Answer Interpreter}
\label{sec:answer_interpreter}
\input{answer_interpreter}


\section{Vectorifier}
\label{sec:vectorifier}
\input{vectorifier}


\section{Symptom Tree Mapper}
\label{sec:symptom_tree_mapper}
\input{symptom_tree_mapper}


\section{How to evaluate predictions?}
\label{sec:eval_results}
Once predictions are computed, the objective is evaluating them. The evaluation process is contextual to the sentence and it is based on a list of $m$ predictions ($[CUI_{p}1, CUI_{p}2, ..., CUI_{p}m]$) and a list of $n$ \emph{real CUIs}, that are the actual symptoms present in the sentence ($[CUI_{s}1, CUI_{s}2, ..., CUI_{s}n]$).

As mentioned in section \ref{sec:cla_symp}, the relationships between every child and every father are ``is-a''. This means that a child symptom is always a specialization of its father. In other words, the child is equal to its father, but more detailed. Thus, a prediction is considered correct if it is located within a subtree rooted in a real CUI.

\begin{figure}[h]
\centering
\includegraphics[width=11cm]{predictions}
\caption{Each real symptom is expanded in a subtree rooted in it. A prediction is evaluated as correct if it stays in one of these subtrees.}
\medskip
\end{figure}

\section{Dataset}
\label{sec:dataset}
%TODO prima persona
As mentioned in section \ref{datasetintro}, in order to create a suitable dataset for this project, I personally extracted and tagged sentences from posts of a medical forum. This forum is named ``Doctors Lounge'' \cite{doctorslounge} and classifies its posts by topic. The extraction and tagging process focused on six categories: chest symptoms; ear, nose and throat symptoms; eyes symptoms; gastroenteric symptoms; headaches; pediatric symptoms and back symptoms.

Extrapolating a sentence within a post means finding a sentence which contains an identifiable and classifiable symptom and tagging it with the relative CUI. The dataset's format is an XML with two types of tags: the \texttt{sentence} tag and the \texttt{symptom} tag (which has an attribute ``\texttt{CUI}'' used for linking the CUI to the symptom text).
