\subsection{Encoding a sentence in a vector}
The first aim of this component is encoding a sentence in a vector. The supported internal representations are two:
\begin{itemize}
  \item \textit{BERT embeddings}. From BERT is possible to extract embeddings, which have the quality of being contextual. For this purpose \texttt{bert-as-a-service} was used \cite{baas}. For extracting embeddings it uses different pooling strategies: with the default one, it does a mean of the vectors of the second-to-last layer. Thus, the dimension of a sentence embedding is $768$, which is the dimension of a single embedding in BERT Base.
  \item \textit{GloVe embeddings}. \textit{GloVe} (Global Vectors for Word Representation) is a method of obtaining pre-trained embeddings from a corpus \cite{glove}. GloVe is different from and better than its precursor, \textit{word2vec}. This because the last one takes only local context into account, considering only words that are inside a window span of the sentence \cite{word2vec}.
  
  On the contrary, GloVe tries both to capture meaning in vector space (like word2vec) and to use global count statistics instead of only local information. GloVe learns word embeddings using a co-occurrence matrix, taking in consideration only nonzero elements.
  
  For the purpose of this work GloVe pretrained embeddings were used. Their dimensions are: 50, 100, 200, 300.
\end{itemize}

In order to find a GloVe representative embedding of a sentence, a mean between the vectorized words in the sentence was done. Not all the words can be vectorified (some of them might be out-of-vocabulary): for this reason the mean takes in consideration only the vectorizable words in the sentence. In order to limit the number of out-of-vocabulary words, a large number of GloVe embeddings ($100\,000$) was imported.

\subsection{Encoding the subsentences of a sentence}
This component does not simply encode a sentence in a vector: it expands the sentence in all its possible sub-sentences, computing the power set of the words in the sentence, and then vectorizes them.

The reason behind this choice is that this procedure provides more flexibility during the mapping of the \textit{tokens for predictions} into symptom concepts.

\begin{figure}[h]
\centering
\includegraphics[width=15cm]{symptom_tree_mapper}
\caption{The Symptom Tree Mapper illustrated}
\medskip
\label{fig:symptom_t_m}
\end{figure}
